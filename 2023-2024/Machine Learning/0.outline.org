#+title: Machine Learning (2022-2023) -- Course Outline
#+author: Jay Morgan
#+date: 04-06-2022

Total delivered time: 10 hours
Delivered via: Lectures
Book accompaniment: Data Mining -- Practical Machine Learning Tools and Techniques
(Fourth Edition), Witten et al.
Programming language used: Julia -- implementation closely follows the mathematical formulae

The course is intended to give students an introduction to Machine Learning,
providing them with firm background in this study should this wish to continue this
research in future and improve the techniques and ideas of the discipline. From this
course, they will be introduced into the concepts of Machine Learning, i.e. what is
learning, and considerations such as bias/variance and
over-fitting/under-fitting. This course will also introduce them into some of the
more fundamental algorithms or techniques, such as decision trees and k-means.

* Lecture 1 - What is Machine Learning?

+ Introduction to course:
  + Who I am.
  + What this course will be about.
  + Where you can find reference material.

+ What does it mean to learn, and how can a machine learn anything?
+ Some terminology -- a model, a dataset (comprised of inputs, and outputs), and
  predictions and ground-truth.
+ A practical example using the classic iris dataset -- we won't solve this just yet,
  it is just a motivating classification problem.
+ Another practical example of predicting boston house prices -- this will come in
  handy for the next section.
+ What can machine learning be used for?
+ Ethical considerations.

* A Machine Learning 'Hello, World!' -- A Linear Regressor

+ Re-introduce the boston housing prices dataset -- what are the input variables and
  output response?
+ A little analysis on the dataset -- usefulness of the variables for the prediction.
+ Choosing one of the input variables to estimate the price.
+ What is a linear regression model? -- straight line equation for prediction.
+ How well does a random linear model do?
+ How do we make the linear model more accurate -- change the slope and intercept.
+ How do we pragmatically select the best weights?
+ Measuring the loss.
+ Visualising the loss w.r.t. slope.
+ Gradient descent algorithm. (what do we mean by convex and non-convex)?
+ Getting the best linear regression model.
+ Closed form solution -- even with the closed-form solution, why might we want to
  use gradient descent? When the dataset is too large for our computers memory.
+ Turning this into a logistic classifier.
+ Adding terms to the linear model.
+ How many terms is too many? -- overfitting & underfitting

* Decision Trees -- rule-based classifiers

+ What is a class/category?
+ Introduction to iris dataset.
+ How might we classify flowers with if-statements?
+ What if we were to visualise these if-statements.
+ An algorithm for identifying rules.
+ Random forests

* Instance-based classifiers - k-nearest neighbour and clustering

+ What is a k-nearest neighbour.
+ Distance functions.
+ Finding nearest neighbours effectively.
+ How do the results compare with different k.
+ What is iterative based clustering.

* Support vector machines

+ Maximum margin intuition
+ Solving non-linear problems using Kernel SVM
+ Soft and hard-svm
+ norm-regularisation

* Bayesian analysis

+ Bayes formula
+ Predicting likelihood with Bayes
+ Bayesian networks

* Evaluation of Machine Learning methods

+ Training, validation, and testing sets
+ Cross-validation (k-fold cross-validation)
+ Precision-recall, ROC curves
+ Confusion matrices
+ Bootstrapping
