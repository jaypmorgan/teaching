<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-11-21 Mon 10:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Machine Learning</title>
<meta name="author" content="Jay Morgan" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="style.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Machine Learning
<br />
<span class="subtitle">Lecture 3 - Evaluating our Models</span>
</h1>

<div id="outline-container-org7161af8" class="outline-2">
<h2 id="org7161af8">Model Performance</h2>
<div class="outline-text-2" id="text-org7161af8">
</div>
<div id="outline-container-org1173a35" class="outline-3">
<h3 id="org1173a35">Train &amp; Test</h3>
<div class="outline-text-3" id="text-org1173a35">
</div>
<div id="outline-container-org6787af8" class="outline-4">
<h4 id="org6787af8">Why do we have different `sets' of data?</h4>
<div class="outline-text-4" id="text-org6787af8">
<p>
We've seen in the previous lecture how we can fit a linear regression model to a set
of data, and we can measure the performance of this model.
</p>

<p>
But we do not understand how well this model works in the `real-world', how well it
performs on data that has not yet been `seen', how well the model <b>generalises</b> to this
unknown data.
</p>

<p>
So, when we want to create a machine learning model, we usually take our data, and
split into two (sometimes three) sets of data. These different sets are named:
</p>

<ul class="org-ul">
<li>training set,</li>
<li>testing set,</li>
<li>and (optionally) validation set.</li>
</ul>
</div>
</div>

<div id="outline-container-org082caf6" class="outline-4">
<h4 id="org082caf6">Training Set</h4>
<div class="outline-text-4" id="text-org082caf6">
<p>
The training dataset, is the set of data, that we're allowing the model to `see' or
learn from.
</p>

<p>
In our example of the linear regression, this is the set of data points to which we
find the optimal parameters of our model.
</p>

<p>
It is not very useful to evaluate our model's performance with the training set as it
doesn't tell us how well it's actually doing (we'll come back to this when we talk
about over-/under-fitting).
</p>
</div>
</div>

<div id="outline-container-org3900496" class="outline-4">
<h4 id="org3900496">Testing Set</h4>
<div class="outline-text-4" id="text-org3900496">
<p>
The testing set is the set of data that we use to evaluate the <b>generalisation</b> of our
machine learning model. The model is not allowed to use this set of data during
training, but it is simply used for the evaluation of the model.
</p>

<p>
In general the testing set is between 10-30% of the overall available data, but this
rule is not something dictated, and may vary depending on the amount of data
available and the overall use case.
</p>

<p>
Once the 10-30% of the data has been sampled for the testing set, the rest of the
data can be used for the training and validation sets.
</p>
</div>
</div>

<div id="outline-container-org4da83d9" class="outline-4">
<h4 id="org4da83d9">Validation Set</h4>
<div class="outline-text-4" id="text-org4da83d9">
<p>
If we have an iterative optimisation process (such as what we saw with gradient
descent), we might want to know how well our model is possibly generalising to unseen
data.
</p>

<p>
The validation dataset, is the set of data that we use to measure the generalisation
of our model during the course of its learning process. Like the test set, this
validation data should not be used to train the model, but only used to measure the
model's generalisation during the lifetime of the learning process.
</p>
</div>
</div>
</div>

<div id="outline-container-org089a569" class="outline-3">
<h3 id="org089a569">Over-/Underfitting</h3>
<div class="outline-text-3" id="text-org089a569">
</div>
<div id="outline-container-org65e304f" class="outline-4">
<h4 id="org65e304f">The ability of the model</h4>
<div class="outline-text-4" id="text-org65e304f">
<p>
When we created a linear regression model, we saw that it was not possible to
predict the house price exactly, there was always some error that we could not
overcome with the linear model.
</p>

<p>
If we have a model complicated model, such as polynomial regression (where we have
polynomial terms in line equation), it may be possible to fit every training data
point exactly. But <b>is that what we want?</b>.
</p>

<p>
In this section, we'll explore the concept of over- and under-fitting, and how we can
use the testing set to understanding if these processes are happening.
</p>
</div>
</div>

<div id="outline-container-org8a931ea" class="outline-4">
<h4 id="org8a931ea">Over-fitting</h4>
<div class="outline-text-4" id="text-org8a931ea">
<p>
We'll begin with over-fitting. Over-fitting occurs when our model has a very high or
perfect performance on the testing set, but does not perform well at all on the
testing set.
</p>

<p>
There may be many reasons for this happening, such as the model being very complex,
having too many variables.
</p>


<div id="org99a2d42" class="figure">
<p><img src="images/overfit.png" alt="overfit.png" width="500px" />
</p>
<p><span class="figure-number">Figure 1: </span>Example of a well-fit model (red dashed) and a model that has overfitted to each data point (blue solid).</p>
</div>
</div>
</div>

<div id="outline-container-orgb20f786" class="outline-4">
<h4 id="orgb20f786">Under-fitting</h4>
<div class="outline-text-4" id="text-orgb20f786">
<p>
Under-fitting, as the name suggests is what happens when we cannot fit the model to
the data, it doesn't even perform well on the training data, the data the model is
allowed to learn from. This can happen when the model is too simple and cannot learn
the intrinsic relationship between the input and output. For example, trying to use a
linear model to learn from data that is not linear by nature.
</p>


<div id="org319a1c5" class="figure">
<p><img src="images/underfit.png" alt="underfit.png" width="500px" />
</p>
<p><span class="figure-number">Figure 2: </span>A linear model (blue line) is unable to properly capture the relationship of this polynomial and so will underfit the data.</p>
</div>
</div>
</div>

<div id="outline-container-org724aefd" class="outline-4">
<h4 id="org724aefd">Validation Curves</h4>
<div class="outline-text-4" id="text-org724aefd">
<p>
If we have an iterative learning process, we can use the training and validation
datasets to measure whether our model is over-fitting, and stop training the model at
an optimal point before it overfits.
</p>

<p>
To do this, at every iteration of the learning process, we evaluate the model's
performance using both the training and validation datasets. If the performance on
both datasets is decreasing we can infer that the model is learning something useful
that helps with it's generalisation to unseen data.
</p>

<p>
However, if the performance on the training set is decreasing, while the performance
on the validation dataset is no longer decreasing or indeed increasing, we know the
model is over-fitting to the training data.
</p>


<div id="orgf98fd32" class="figure">
<p><img src="images/validation_curves.png" alt="validation_curves.png" width="500px" />
</p>
<p><span class="figure-number">Figure 3: </span>Example of training and validation loss as the model iteratively trains. Both the train and validation loss decreases, up until a point where the model begins overfitting. This overfitting begins to occur when the training loss continues to decrease but the validation loss either stops decreasing or increases. Ideally, we would use the model parameters from the model with the lowest validation loss.</p>
</div>
</div>
</div>

<div id="outline-container-orgbcb6a4a" class="outline-4">
<h4 id="orgbcb6a4a">Bias/Variance Tradeoff</h4>
<div class="outline-text-4" id="text-orgbcb6a4a">
<p>
The Bias/Variance tradeoff is a property of machine learning models. It describes a
model's expected generalisation abilities based upon how the parameters are estimated
across a dataset.
</p>

<ul class="org-ul">
<li><b>Bias</b> - the model's `flexibility' to represent the true relationship in the
data. Model's with low bias have a tendency to underfit. An example would be a
linear model model trying to fit against non-linear function.</li>
<li><b>Variance</b> - the impact that a single sample in the data has on the model. Model's
with high variance tend to overfit to the training data.</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org428447c" class="outline-2">
<h2 id="org428447c">Regression</h2>
<div class="outline-text-2" id="text-org428447c">
</div>
<div id="outline-container-orgf6fa24e" class="outline-4">
<h4 id="orgf6fa24e">Metrics</h4>
<div class="outline-text-4" id="text-orgf6fa24e">
<p>
Now that we've looked at the various sets of data, and the potential scenarios when
we fit a model, we'll now want to look at some actual methods of evaluating our
model.
</p>

<p>
These we call <b>metrics</b>. Metrics are values that help us understand how well a model
might perform in the real world. <b>Metrics are helpful to explain the predictive power
of a model with one value</b>.
</p>

<p>
There are many different types of metrics that can be used depending on the class of
problem that is being dealt with. For instance, there are different set of metrics
for Regression and classification problems.
</p>

<p>
We'll first look at some metrics we can use to evaluate a regression model (some of
which we've already seen in the Linear models lecture), and then we'll have a look at
metrics for a classification task.
</p>
</div>
</div>

<div id="outline-container-orge6f7b8b" class="outline-4">
<h4 id="orge6f7b8b">Mean Squared Error (MSE)</h4>
<div class="outline-text-4" id="text-orge6f7b8b">
<p>
A mean squared error (sometimes called the sum of squared residuals) is the measure
of mean magnitude between two sets of points \(y, \overline{y}\).
</p>

<p>
The formula for MSE is:
</p>

<p>
\(\text{MSE} = \frac{1}{N} \sum_i^N (y_i, \overline{y_i})^2\)
</p>

<p>
for \(N\) points.
</p>

<p>
MSE is always non-negative, and the lower the MSE the better.
</p>
</div>
</div>

<div id="outline-container-org1e2a23f" class="outline-4">
<h4 id="org1e2a23f">Root Mean Squared Error (RMSE)</h4>
<div class="outline-text-4" id="text-org1e2a23f">
<p>
\[
\text{RMSE} = \sqrt{\text{MSE}}
\]
</p>

<p>
Due to the squared error term, larger errors have a large effect on the outcome of
the equation, so both RMSE and MSE is sensitive to outliers.
</p>

<p>
MSE's error is measured in squared units, while RMSE is measured in the same unit as
the target.
</p>
</div>
</div>

<div id="outline-container-org6b404bd" class="outline-4">
<h4 id="org6b404bd">Mean Absolute Error (MAE)</h4>
<div class="outline-text-4" id="text-org6b404bd">
<p>
Mean absolute error or MAE is one objective function for measure the \(L_1\) between two sets of points.
</p>

<p>
\(\text{MAE} = \frac{1}{N} \sum_i^N | y_i - \overline{y_i} |\)
</p>

<p>
for \(N\) of points.
</p>

<p>
Like MSE, RMSE, the lower the MAE value, the better the fit on the statistical model.
</p>
</div>
</div>
</div>

<div id="outline-container-org0ed06e6" class="outline-2">
<h2 id="org0ed06e6">Classification</h2>
<div class="outline-text-2" id="text-org0ed06e6">
</div>
<div id="outline-container-orgc6c626b" class="outline-4">
<h4 id="orgc6c626b">Binary classification &amp; labelling as positive or negative</h4>
<div class="outline-text-4" id="text-orgc6c626b">
<p>
We now move onto some of the more typical classification metrics. But first, we must
first understand when our classifier predicts positive or negative in a binary
classification task.
</p>

<p>
Let's say we have a binary classifier \(\mathcal{M}\) which predicts the positive class
if the predicted probability is \(\geq 0.5\). I.e.:
</p>

<p>
\[
L(x)  = \begin{cases}
1 & \text{if}, \; \mathcal{M}(x) \geq 0.5 \\
0 & \text{otherwise}
\end{cases}
\]
</p>

<p>
where \(L\) is our labelling function.
</p>

<p>
Here 0.5 is the <b>threshold</b> for predicting the positive class.
</p>
</div>
</div>

<div id="outline-container-orgc4eba5b" class="outline-4">
<h4 id="orgc4eba5b">TP/TN/FP/FN</h4>
<div class="outline-text-4" id="text-orgc4eba5b">
<p>
Before we look at other metrics to evaluate our classification metrics, I first want to describe these acronyms.
</p>
</div>

<ul class="org-ul">
<li><a id="org6d45697"></a>List&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_column">B_column</span>&#xa0;<span class="BMCOL">BMCOL</span></span><br />
<div class="outline-text-5" id="text-org6d45697">
<ul class="org-ul">
<li>TP - True-Positive &#x2013; our model has predicted positive (it was correct) and the
actual label is positive.</li>
<li>TN - True-Negative &#x2013; our model has predicted negative (it was correct) and the
actual label is negative.</li>
<li>FP - False-Positive &#x2013; our model has predicted positive (it was wrong) the actual
label was negative.</li>
<li>FN - False-Negative &#x2013; our model has predicted negative (it was wrong) the actual
label was positive.</li>
</ul>
</div>
</li>

<li><a id="orgc4d8b3f"></a>Diagram&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_column">B_column</span>&#xa0;<span class="BMCOL">BMCOL</span></span><br />
<div class="outline-text-5" id="text-orgc4d8b3f">

<div id="org65578c4" class="figure">
<p><img src="images/Precisionrecall.png" alt="Precisionrecall.png" />
</p>
<p><span class="figure-number">Figure 4: </span>By Walber - Own work, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=36926283">https://commons.wikimedia.org/w/index.php?curid=36926283</a></p>
</div>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgc0b70ed" class="outline-4">
<h4 id="orgc0b70ed">Accuracy</h4>
<div class="outline-text-4" id="text-orgc0b70ed">
<p>
In a binary classification task, accuracy is measured using:
</p>

<p>
\[
\text{Accuracy} = \frac{TP+TN}{TP+TN+FP+FN}
\]
</p>

<p>
or multi-classification:
</p>

<p>
\[
\text{Accuracy} = \frac{\text{number of correct}}{\text{number of samples}}
\]
</p>

<p>
The range of accuracy is in \([0, 1]\), the higher the value of accuracy the
better. Accuracy is often presented in the form of a percentage i.e. \(100 \cdot \text{Accuracy}\)
</p>
</div>
</div>

<div id="outline-container-org39b5360" class="outline-4">
<h4 id="org39b5360">Precision</h4>
<div class="outline-text-4" id="text-org39b5360">
<p>
Measuring the precision tells us how many how accurate our model was in predicting positive cases. Here we have \(TP\) or the number of True-Positive, divided by \(TP + FP\) where \(FP\) is the number of False-Positive cases.
</p>

<p>
\[
\frac{TP}{TP + FP}
\]
</p>

<p>
Valid values for the precision metric are in the range \([0, 1]\) where the higher the
value the better.
</p>
</div>
</div>

<div id="outline-container-org75ef0ca" class="outline-4">
<h4 id="org75ef0ca">Recall</h4>
<div class="outline-text-4" id="text-org75ef0ca">
<p>
Recall tells us: out of all the positive cases, how many of them were actually
found/predicted to be positive. How many of these positive cases was our model able
to <i>recall</i>?
</p>

<p>
\[
\frac{TP}{TP+FN}
\]
</p>

<p>
Like precision, recall is in the range \([0, 1]\) where the higher the value the better
the recall.
</p>
</div>
</div>

<div id="outline-container-org080ba41" class="outline-4">
<h4 id="org080ba41">Confusion Matrix</h4>
<div class="outline-text-4" id="text-org080ba41">
<p>
A confusion matrix is a visual representation of these different type of predictive
cases (TP/TN/FP/FN).
</p>

<p>
An optimal confusion matrix, is a diagonal matrix (all entries outside of the
diagonal are zero). Here is one example of a confusion matrix.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">Predicted</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">Positive</td>
<td class="org-right">Negative</td>
</tr>

<tr>
<td class="org-left">Actual</td>
<td class="org-left">Positive</td>
<td class="org-right">5</td>
<td class="org-right">2</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">Negative</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>

<ul class="org-ul">
<li><a id="orgfd608ca"></a>Values&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_column">B_column</span>&#xa0;<span class="BMCOL">BMCOL</span></span><br />
<div class="outline-text-5" id="text-orgfd608ca">
<ul class="org-ul">
<li>TP = 5</li>
<li>TN = 1</li>
<li>FP = 3</li>
<li>FN = 2</li>
</ul>
</div>
</li>

<li><a id="org9d62159"></a>Calculation&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_column">B_column</span>&#xa0;<span class="BMCOL">BMCOL</span></span><br />
<div class="outline-text-5" id="text-org9d62159">
<ul class="org-ul">
<li>Precision = \(\frac{5}{5+3} = 0.625\)</li>
<li>Recall = \(\frac{5}{5+2}\) \(= 0.714\)</li>
</ul>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgf33c581" class="outline-4">
<h4 id="orgf33c581">\(F_\beta\) &amp; \(F_1\)</h4>
<div class="outline-text-4" id="text-orgf33c581">
<p>
With the precision/recall metrics, it is trivial to optimise for one over the over:
</p>

<ul class="org-ul">
<li>We can achieve perfect precision (\(\text{precision} = 1\)) by predicting everything
is negative (no false positives)</li>
<li>We can achieve perfect recall (\(\text{recall} = 1\)) by predicting that everything
is positive (no false negatives).</li>
</ul>

<p>
But predicting everything is negative, or everything is positive is not really a
useful model. So we have another metric that is the <b>harmonic combination</b> of precision
and recall: \(F_1\) and \(F_\beta\) score.
</p>

<p>
\[
F_\beta = (1 + \beta^2) \frac{p \cdot r}{\beta^2 p + r}
\]
</p>

<p>
where \(p, r\) is the precision and recall metric respectively. For the \(F_1\) score, we
simple set \(\beta = 1\).
</p>
</div>
</div>

<div id="outline-container-orgc8467ef" class="outline-4">
<h4 id="orgc8467ef">Receiver Operating Characteristic (ROC)</h4>
<div class="outline-text-4" id="text-orgc8467ef">
<p>
In the previous slides, we have labelled our samples as positive if our classifier
predicts \(\geq 0.5\), else it is labelled as negative. This \(0.5\) is our threshold for
our labelling function. But we can vary this threshold if we want. Lowering the
threshold will typically mean our classifier labels positive cases more often. While
increasing the threshold makes the classifier more conservative, and typically
predicts labels positive cases less often.
</p>

<p>
If we vary this threshold from 0 to 1 and calculate the true- and false-positive
rate, we can visualise something we call the <b>Receiver Operating Characteristic</b> or ROC for
short.
</p>


<div id="orga4b1598" class="figure">
<p><img src="images/roc.png" alt="roc.png" width="500px" />
</p>
</div>

<p>
This ROC curve, with the dotted line directly in the centre, first shows us what a
random classifier would look like. This random classifier randomly predicts positive
or negative for any case.
</p>

<p>
We can say that our classifier is better than random, if the line is to the <b>top-left</b>
of the random classifier. In general, the more to the top-left the line is, the better.
</p>
</div>
</div>

<div id="outline-container-org5bbb091" class="outline-4">
<h4 id="org5bbb091">Area Under Curve (AUC)</h4>
<div class="outline-text-4" id="text-org5bbb091">
<p>
We've just seen how the ROC curve can visually point to which model is better than
others, and which threshold we may want to choose for our labelling
function. However, we can also turn these types of curves in a number, a metric.
</p>

<p>
This next metric we're going to look at does just that. The <b>Area Under Curve</b> or AUC
for short, takes our ROC curve, and measures the area underneath the curve, giving us
a single value for each model that we can use for comparison.
</p>


<div id="orgcb9de92" class="figure">
<p><img src="images/auc.png" alt="auc.png" width="500px" />
</p>
</div>

<p>
One method to calculate this area is to use the trapezoid rule to approximate the
region underneath the graph of a function:
</p>

<p>
\[
\text{Area} = 0.5 \frac{1}{N} \times \left[ \text{TP}_1 + 2 (\text{TP}_2 + ... + \text{TP}_{N-1}) + \text{TP}_N \right]
\]
</p>

<p>
If the AUC is close to one we know that the model at any threshold has very good
discriminatory power.
</p>
</div>
</div>
</div>

<div id="outline-container-orgb9df687" class="outline-2">
<h2 id="orgb9df687">Choosing the best model</h2>
<div class="outline-text-2" id="text-orgb9df687">
</div>
<div id="outline-container-orgd6b3fab" class="outline-4">
<h4 id="orgd6b3fab">Cross Validation using K-fold</h4>
<div class="outline-text-4" id="text-orgd6b3fab">
<p>
We have seen why having a separate set of data for training, testing, and validation
is necessary &#x2013; to give some indication as to the generalisation performance of our
model, and to track possible over-fitting.
</p>

<p>
To create these separate sets of data, we may have just sampled randomly or using a
stratified method (more on this in a few slides). However, this is only one test of
the model's generalisation abilities.
</p>

<p>
Cross-validation is a statistical method to test the model on many <b>resamplings</b> on the
test set.
</p>


<div id="orgd5466fe" class="figure">
<p><img src="images/K-fold_cross_validation_EN.png" alt="K-fold_cross_validation_EN.png" />
</p>
<p><span class="figure-number">Figure 5: </span>K-fold cross-validation (By Gufosowa - Own work, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=82298768">https://commons.wikimedia.org/w/index.php?curid=82298768</a>)</p>
</div>

<p>
Cross-validation works selecting a subset of the data for testing (leaving the rest
for training), training the model, and then calculating the performance on this test
set. Next, sample a different subset of data for a new testing set, training the
model, and calculating the performance. Repeat this process until all data has been
sampled for the testing set, and calculate the mean and standard deviation of model
performance.
</p>

<p>
<b>K-fold</b> cross-validation is this method where \(k\) is the number of iterations it will
take to have used the entire available data for testing. I.e., if you're performing
5-fold cross-validation, you would have trained and tested your model 5 different
types, on 5 different samples from your available data.
</p>
</div>
</div>

<div id="outline-container-org032b5a5" class="outline-4">
<h4 id="org032b5a5">Random &amp; Stratified Sampling</h4>
<div class="outline-text-4" id="text-org032b5a5">
<p>
When sampling data for our training and testing set, we could use two different
methods:
</p>

<ul class="org-ul">
<li>Random</li>
<li>Stratified</li>
</ul>

<p>
To perform stratified sampling, we first split the dataset into stratas or distinct
groups. For a classification problem, this could be splitting samplings by their
respective class labels. Then, after splitting the data into their respective groups,
we randomly sample from each group.
</p>

<p>
Let's say we have 150 samples, where:
</p>

<ul class="org-ul">
<li>40 samples are in group 1,</li>
<li>25 samples are in group 2,</li>
<li>85 samples are in group 3.</li>
</ul>

<p>
And we want to sample from this dataset for our test set using stratified
sampling. First, we calculate the proportion of each group in the overall data:
</p>

<ul class="org-ul">
<li>\(100 \times \frac{40}{150} = 26.\overline{6}\; \%\),</li>
<li>\(100 \times \frac{25}{150} = 16.\overline{6}\; \%\),</li>
<li>\(100 \times \frac{85}{150} = 56.\overline{6} \; \%\).</li>
</ul>

<p>
Therefore, in our testing set, \(26.\overline{6} \; \%\) of the data should be randomly
sampled from group 1, and so on for all groups.
</p>

<p>
So if we want to use \(10 \; \%\) of our data for testing, that means we would have 15
samples in our dataset (\(\frac{150}{10}\)) sampled using:
</p>

<ul class="org-ul">
<li>group 1: \(40 \times (\frac{15}{150}) = 4\) samples,</li>
<li>group 2: \(25\times (\frac{15}{150}) = 2.5\) samples,</li>
<li>group 3: \(85 \times ( \frac{15}{150}) = 8.5\) samples.</li>
</ul>

<p>
The proportion of samples in our test set from each group should roughly match the
proportion of the overall available data. We can verify this by calculating the
proportion of each group's representation, i.e. : \(100 \times \frac{4}{15} =
26.\overline{6} \; \%\) and we see that it matches the proportion of the overall data.
</p>

<p>
Stratified sampling is especially useful when we have a class-imbalance, and randomly
sampling data could potentially lead to a situation where our test or training set
only has one class label.
</p>
</div>
</div>
</div>

<div id="outline-container-org949d1c3" class="outline-2">
<h2 id="org949d1c3">Summary</h2>
<div class="outline-text-2" id="text-org949d1c3">
</div>
<div id="outline-container-org2793596" class="outline-4">
<h4 id="org2793596">Summary</h4>
<div class="outline-text-4" id="text-org2793596">
<p>
In this lecture we have discussed the following things:
</p>

<ul class="org-ul">
<li>The idea behind having different subsets of data dedicated to training, testing,
and validation.</li>
<li>Different metrics that are used to evaluation regression and classification models.</li>
<li>Various methods to sample data using cross-validation, random, or stratified sampling.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 16th November</p>
<p class="author">Author: Jay Morgan</p>
<p class="date">Created: 2022-11-21 Mon 10:35</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
