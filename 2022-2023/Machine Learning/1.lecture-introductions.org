#+title: Machine Learning
#+SUBTITLE: Lecture 1 - Introductions
#+author: Jay Morgan
#+date: <TODO>
#+startup: beamer
#+BIBLIOGRAPHY: references.bib
#+LaTeX_CLASS: beamer
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{pmboxdraw}
#+LATEX_HEADER: \usetheme{Berkeley}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \definecolor{UBCblue}{rgb}{0.54706, 0.13725, 0.26667} % UBC Blue (primary)
#+LATEX_HEADER: \usecolortheme[named=UBCblue]{structure}
#+LATEX_HEADER: \setlength{\parskip}{5pt}
#+LATEX_CLASS_OPTIONS: [10pt]
#+LATEX_HEADER: \newcommand{\footnoteframe}[1]{\footnote[frame]{#1}}
#+LaTeX_HEADER: \addtobeamertemplate{footnote}{}{\vspace{2ex}}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \definecolor{LightGray}{gray}{0.95}
#+OPTIONS:   H:3 num:nil toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />
#+PROPERTY: header-args:julia :exports both :results output :eval never-export :session

* Introduction

** Course Organisation

*** Welcome!

Welcome to all the new students! Here I am going to be talking about Machine Learning
and all of the great things that this "technology" has to offer. To begin our course,
I shall start with a bit of house keeping -- more specifically, I will be talking
about what exactly we'll be learning about in the course (Machine Learning is a broad
subject after-all). In addition, I will tell you where you can find the resources
related to the course and how you can contact me, should you have any questions.

*** What this course is about?

In this course, we will be learning about Machine Learning: firstly, what Machine
Learning actually is; secondly, we'll take a look at some of the algorithms within
the scope of Machine Learning, and develop an intuition about how these algorithms
work and when they would be useful; and finally, how we can compare and evaluate the
algorithms we've learnt about.

*** How this course will be taught

I intended to deliver this course via a series of lectures. These lectures will be
accompanied by the PDF lecture slides, in which I will provide the definitions and
provide reference links should you wish to do some extra reading.

*** Outline of the course

#+ATTR_LATEX: :booktabs t
|---------+---------------+---------+--------------|
| Lecture | Expected date | Length  | Topic        |
|---------+---------------+---------+--------------|
|       1 | NOW           | 2 hours | Introduction |
|         |               |         |              |
|---------+---------------+---------+--------------|

** Reading the lectures

*** Source code

During the course, I would also like to supplement my algorithmic definitions and
explanations with some programming code -- for this I will use the [[https://julialang.org/][Julia]] programming
language. The code snippets would look something like:

#+begin_src julia :results output :session :exports both
x = Float32.([1, 2, 3, 4]);
y = x .+ randn(length(x))
#+end_src

#+RESULTS:
: 4-element Vector{Float64}:
:  1.8817576050704024
:  2.051062476034138
:  4.340649570113399
:  3.1284623590241476

*** References

In some cases, and is the norm with academic traditions, we'll want to include a
reference, a link to previous written works.

Here is an example of a sentence that includes a reference:

"This is a very important sentence which I assert to be true, to convince you of this
fact I shall appeal to authority and include a reference:
[cite:@shalev2014understanding]"

More information on the referenced material (such as title, publishing date) will be
found in the bibliography slide (or bottom of the webpage if you're viewing the HTML
version of the lectures).

*** About Me

My name is Dr Jay Morgan. I am a researcher at the Université de Toulon, where I am
developing Deep Learning models (a sub-field of Machine Learning research) for the
study of astrophysical phenomenon.

You can find more information and links on my personal (LIS-Lab) website:
https://pageperso.lis-lab.fr/jay.morgan/

I also publish libraries and source code online:
- Github: https://github.com/jaypmorgan
- Gitlab: https://gitlab.com/jaymorgan
- Source Hut: https://sr.ht/~jaymorgan/

If you have any questions, you can email me at jay.morgan@univ-tln.fr

*** Where you can find the resources

I try to make this course as accessible as possible, which means that I host these
slides in a variety of ways to suit you.

Firstly, you can find the links to all my courses on my personal website at:
https://pageperso.lis-lab.fr/jay.morgan/teaching.html

Here you can find the links to each lecture in a PDF or HTML format. Additionally,
you can view the source code used to make these lectures on source hut:
https://git.sr.ht/~jaymorgan/teaching. On this git repository you can find all my
lectures from all years.

* What is learning, anyway?

** Learning about Learning

*** Let's answer the question of learning

We'll begin our journey into the world of Machine Learning by tackling the question
of what it means to 'learn' -- how may a machine actually /learn/ anything?

*** Study of Mice

To begin to answer the question of learning, we may turn to nature for
advice. Principally, if we look at the studies conducted with Mice we find some idea
to notion of learning [cite:@shalev2014understanding].

*** Bait-shyness

When a rat encounters a novel source of food, it will first eat a little bit of
it. If the food is edible for the rat, it will continue to eat the food, even in
future encounters. If, however, on the initial contact with the food, the rat deems
the food poisonous, it will ignore and not eat the food in future encounters. This
process we call 'bait-shyness'.

Here then we see the rat, on finding something new, learn from its experience, and
use that knowledge of the experience for future encounters.

*** Bait-shyness

Our initial understanding of rat's bait-shyness was limited, but we've come to
understand more about it. For instance, we learn that their learning process is more
complex than originally thought. In a later experiment, where the 'poison' in the
food is replaced by a different unpleasant stimulus such as a electric shock --
i.e. when a rat eats a food, it is then shocked. It was found that this did not
deter the rat from eating the food in future encounters, unlike the poison.

It is presumed that the rat's have some 'prior knowledge' about the world and do not
infer a temporal relationship between the food and being shocked, while they can
infer the same relationship with food and illness.

*** Computer Programs

From these two examples of how rats may learn we see: the rat will make a guess about
something now (i.e. that the food is not poisonous), it will find out how good this
guess is (i.e. it either gets ill or it does not), and learn from how well its guess
was for the future. We also see that its learning can be impacted by the rat's prior
knowledge about how the world may work.

But how does this /framework/ for the process of learning translate to computers? For a
more formal definition of how computer programs could be said to learn, we have a
similar idea:

#+begin_quote
A computer program is said to learn from experience $E$ with respect
to some class of tasks $T$ and performance measure $P$, if its performance
a tasks in $T$, as measured by $P$, improves with experience $E$.
#+end_quote

[cite:@mitchell1997machine]

*** When might we need Machine Learning

Why do we need computer programs that 'learn' anyway? We already have programming
languages, why can't we just use them?

Let's suppose we're creating a very simple Optical Character Recognition (OCR)
program. This program looks at a PDF document and converts the text into something we
can copy and paste. Part of this program's task is to take an individual character,
say the number '8', and recognise that it's an 8 and add that to the already scanned
text.

How would we go about creating a program where we can define how to identify '8' or
'1' or 'l' -- with all the varieties of lighting conditions, handwriting, fonts,
sizes. We could find the process of encompassing all different variations tiresome --
if not impossible, and that's only for a single character!

*** When might we need Machine Learning

With Machine Learning, instead of enumerating all possible solutions within a
programming language, we collect a bunch of examples of '8's and give them to the
algorithm to learn from.

Through looking at these many different examples, the algorithm will/should be able
to recognise what an 8 generally looks like.

*** Different types of Learning

What we have just demonstrated by way of the OCR example, is the type of learning we
call 'Supervised Learning'. We have many examples of input (lots of different kinds
of handwritten 8's), and we tell the learning algorithm, that they are indeed the
number 8.

But there are other kind of different learning frameworks. Specifically we have the
following:

- Supervised Learning
- Unsupervised, or sometimes called self-supervised Learning
- Reinforcement Learning

*** Supervised Learning

To better formalise Supervised Learning from our previous OCR example, Supervised
Learning is when the learning algorithm "see's" or has access to both the input and
output.

Let's have a dataset $X$, which is a set consisting of tuple pairs $x_i, y_i$. $x_i£$
is an input, i.e. a single image with an '8', and $y_i$ is a label which tells the
learning algorithm if the input is indeed an '8' or something else. Mathematically we have:

$X = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$

*** Unsupervised Learning

In Unsupervised learning, we have again have a dataset $X$, who's elements are only
inputs. In other words, there are no corresponding labels for each input. Instead,
the learning algorithm must learn inherent patterns in the data and create labels
itself. Throughout the course, we'll see examples of Unsupervised Learning in action.

One thing to note: Recent methodologies have started to call Unsupervised Learning,
self-supervised. As we have just discussed, the labels are inherent to the data from
the discovered patterns, it's just we are not explicitly giving them to the learning
algorithm ourselves. So it's sort of like a supervised learning setup, except the
learning algorithm is providing the labels itself -- hence the self-supervised.

*** Reinforcement Learning

Reinforcement Learning is very different to both Supervised and Unsupervised
Learning. Here is the type of learning you might be familiar with if you've seen 'AI'
that learns to play video games. In this type of learning, we have the following
elements:

- An agent
- An environment
- A set of allowed actions the agent can make within its environment.

In this situation, an agent will interact with it's environment, and when it does
something it can receive a reward (a reward can be positive or negative). The agent
will remember what it has done to receive the reward. The objective for the agent is
to maximise the reward score, and learns to do this through many iterations or
play-through.

** Terminology

*** What will our data look like?

Data in Machine Learning applications can come in a variety of different formats. The
most typical data formats we might see are:

- Tables
- Images/Videos
- Text
- Sound

These are the initial formats, though, before actually doing any learning, we will
want to transform them into a different representation that we can use.

*** Tables

A table, or tabular, format is a $n \times m$ set of data with $n$ samples or
examples, and $m$ features for each sample. For example, suppose we have a table
consisting the price of 100 different houses:

#+ATTR_LATEX: :booktabs t
|--------------------+------------------+-----+------------|
| Number of bedrooms | Garden size (ft) | ... | Price ($)  |
|--------------------+------------------+-----+------------|
|                  3 |                0 | ... | 150,000    |
|                  5 |               10 | ... | 200,000    |
|                ... |              ... | ... |            |
|                 10 |             1000 | ... | 2,000,000  |
|--------------------+------------------+-----+------------|

In a supervised learning setting, where we want to predict the price of a house we may then have the following dataset:

$X = \{([3, 0, ...], 150,000), ([5, 10, ...], 200,000), \\..., ([10, 1000, ...], 2,000,000)\}$

*** Images/Videos

Images are composed of 2D or 3D arrays of numeric values. For example, in a RGB
image that is 1024x500 pixels, we would have the array of size 1024x500x3 -- where 3
is the red, green, and blue channel, respectively. If we have just a grayscale image,
we could represent it as either 1024x500x1 or 1024x500 as the channel 'dimension' of
the array is singular.

We may already know that videos are simply a sequence of images that are iterated
through 24+ times a second. For a 24 frames per second video, we would have an array
size of 1024x500x3x24 -- a 4-dimensional array.

*** Text

- Composition of words
- Can be in order, but doesn't have to be!
- What is considered a word?

*** Time-series

- Sequence of values (vector with a time component)
- Example is a sound wave.

*** Inputs/Outputs

Within our data, each value will take on a certain type.

| Name        | Example                     |
|-------------+-----------------------------|
| Discrete    | Total number of students    |
| Continuious | Height of a student         |
|-------------+-----------------------------|
| Ordinal     | element of {Low, Med, High} |
| Nominal     | element of {'dog', 'cat'}   |

- Discrete:
- Continuious:
- Ordinal: 
- Nominal:

*** Types of Outputs -- Regression & Classification

- Regression - to continuious
- Classification to discrete

** Example Problems

*** Boston House Prices Dataset -- Tabular Regression

#+begin_src julia  :results output :session :exports both
using DataFrames
using MLDatasets: BostonHousing
dataset = BostonHousing();
dataset[1:5]
#+end_src

#+RESULTS:
#+begin_example
(features = 5×13 DataFrame
 Row │ CRIM     ZN       INDUS    CHAS   NOX      RM       AGE      DIS      RAD    TAX    PTRATIO  B        LSTAT   
     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64  Int64  Float64  Float64  Float64 
─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1    296     15.3   396.9      4.98 
   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2    242     17.8   396.9      9.14
   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2    242     17.8   392.83     4.03
   4 │ 0.03237      0.0     2.18      0    0.458    6.998     45.8   6.0622      3    222     18.7   394.63     2.94
   5 │ 0.06905      0.0     2.18      0    0.458    7.147     54.2   6.0622      3    222     18.7   396.9      5.33 , targets = 5×1 DataFrame
 Row │ MEDV
     │ Float64
─────┼─────────
   1 │    24.0
   2 │    21.6
   3 │    34.7
   4 │    33.4
   5 │    36.2)
#+end_example

*** Iris Dataset -- Tabular Classification

#+begin_src julia
using MLDatasets: Iris
dataset = Iris();
dataset[1:5]
#+end_src

#+RESULTS:
#+begin_example
(features = 5×4 DataFrame
 Row │ sepallength  sepalwidth  petallength  petalwidth
     │ Float64      Float64     Float64      Float64
─────┼──────────────────────────────────────────────────
   1 │         5.1         3.5          1.4         0.2
   2 │         4.9         3.0          1.4         0.2
   3 │         4.7         3.2          1.3         0.2
   4 │         4.6         3.1          1.5         0.2
   5 │         5.0         3.6          1.4         0.2, targets = 5×1 DataFrame
 Row │ class
     │ String15
─────┼─────────────
   1 │ Iris-setosa
   2 │ Iris-setosa
   3 │ Iris-setosa
   4 │ Iris-setosa
   5 │ Iris-setosa)
#+end_example

*** MNIST Dataset -- Image Classification

#+begin_src julia
using MLDatasets: MNIST
dataset = MNIST();
dataset[1:5]
#+end_src

#+RESULTS:
: (features = [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = [5, 0, 4, 1, 9])

*** MNIST Dataset -- Image Classification

#+begin_src julia
using Plots
p = heatmap(dataset[1].features', yflip=true, colorbar=false)
p = title!("Label/Target: $(dataset[1].targets)")
savefig(p, "images/mnist-example.png")
#+end_src

#+RESULTS:
#+begin_example



ERROR: SystemError: opening file "/home/jaymorgan/Nextcloud/Teaching/images/mnist-example.png": Aucun fichier ou dossier de ce type
Stacktrace:
  [1] systemerror(p::String, errno::Int32; extrainfo::Nothing)
    @ Base ./error.jl:174
  [2] #systemerror#68
    @ ./error.jl:173 [inlined]
  [3] systemerror
    @ ./error.jl:173 [inlined]
  [4] open(fname::String; lock::Bool, read::Nothing, write::Nothing, create::Nothing, truncate::Bool, append::Nothing)
    @ Base ./iostream.jl:293
  [5] open(fname::String, mode::String; lock::Bool)
    @ Base ./iostream.jl:355
  [6] open(fname::String, mode::String)
    @ Base ./iostream.jl:355
  [7] open(::Plots.var"#277#278"{Plots.Plot{Plots.GRBackend}}, ::String, ::Vararg{String}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
    @ Base ./io.jl:328
  [8] open
    @ ./io.jl:328 [inlined]
  [9] png(plt::Plots.Plot{Plots.GRBackend}, fn::String)
    @ Plots ~/.julia/packages/Plots/SkUg1/src/output.jl:4
 [10] savefig(plt::Plots.Plot{Plots.GRBackend}, fn::String)
    @ Plots ~/.julia/packages/Plots/SkUg1/src/output.jl:115
 [11] top-level scope
    @ none:1
#+end_example

*** IMDB Reviews -- Text Classification/Regression

*** Ham or Spam -- Text Classification

** Concerns & Considerations

*** Compute resources -- environmental concerns

*** Bias

*** Personal information

- i.e. GANs reproducing exactly.

*** Mental health of optimisation algorithms

*** Copyright Concerns

Potential code laundering with github's copilot

* Summary

** What is Machine Learning

*** Problem statement

* Bibliography

*** Bibliography

#+PRINT_BIBLIOGRAPHY:

# Local Variables:
# org-latex-minted-options: (("frame" "lines") ("linenos=true") ("firstnumber=last") ("fontsize=\\footnotesize") ("bgcolor=LightGray") ("xleftmargin=5pt") ("tabsize=2") ("breaklines=true") ("numbersep=10pt"))
# End:
