#+title: Machine Learning
#+subtitle: Lecture 2 - Linear Models
#+author: Jay Morgan
#+date: <TODO>
#+startup: beamer
#+include: ./header.org
#+property: header-args:python :session :eval never-export :results none :exports none :session :dir ./

#+name: savefig
#+begin_src python :results value replace :exports none :var name="/tmp/plot.png"
f"""import matplotlib.pyplot as plt
plt.savefig('{name}', bbox_inches='tight')
plt.close()
'{name}'
"""
#+end_src

#+RESULTS: savefig
: import matplotlib.pyplot as plt
: plt.savefig('/tmp/plot.png', bbox_inches='tight')
: plt.close()
: '/tmp/plot.png'

#+begin_src python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
#+end_src

* Linear Regression

** Introduction to linear models

*** Linear models

Having learnt a little about what it means to learn, we're going to look at our first
/Machine Learning/ algorithm, the staple for much of statistics, numeric prediction
using a linear model.

*** What is a linear model?
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END: 

A linear model is a prediction (a response) to an input variable. We have the
following terms:

- Response/prediction -- the output of the model.
- Dependant variable -- the variable upon which the prediction is being made.

For a linear model based on one dependant we have the following:

\[
y = w x + b
\]

where $y$ is the response/output/prediction of the model, $x$ is the dependant
variable, and $w, b$ are the model parameters.

*** Slope & intercept
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END: 

If we look at our linear model equation, we'll notice that it's the same equation for a straight line.

#+begin_src python :results value file replace :exports results :noweb strip-export
plt.figure()
x = np.arange(10)
y = 2 * x + 3
plt.plot(x, y, 'o-', label="$2x + 3$")
plt.xlabel("Dependant variable $x$")
plt.ylabel("Response $y$")
plt.legend()
<<savefig(name="images/linear_model.png")>>
#+end_src

#+ATTR_ORG: :width 300px
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS:
[[file:images/linear_model.png]]

As we've seen, the linear model, or linear regression, has two parameters: $w,
b$. What do these parameters represent?

- The $w$ parameter is the *slope* or strength of relationship between the dependant
  variable and the response.
- Meanwhile, the $b$ parameter is called the *intercept*, as it's the value of the
  response when the dependant variable is zero.

Let's look at these two parameters.

#+begin_src python :results value file replace :exports results :noweb strip-export
fig, ax = plt.subplots(1, 3, figsize=(8, 3))

x = np.arange(10)

slope = 0
ax[0].plot(x, slope * x + 5, 'o-')
ax[0].set_title(f"${slope}x + 5$")
ax[0].set_ylim(0, 10)

slope = 2
ax[1].plot(x, slope * x + 5, 'o-')
ax[1].set_title(f"${slope}x + 5$")
ax[1].set_ylim(0, 10)

slope = -1
ax[2].plot(x, slope * x + 5, 'o-')
ax[2].set_title(f"${slope}x + 5$")
ax[2].set_ylim(0, 10)

<<savefig(name="images/slope_1.png")>>
#+end_src

#+ATTR_ORG: :width 300px
#+RESULTS:
[[file:images/slope_1.png]]

Here we see that when $w$ is 0 (left figure), any change in $x$ results in 0 change
in $y$. While, with $w = 2$, $y$ increases two-fold by every change in $x$. Finally,
when the slope is negative, we see that $y$ decreases.

Notice how the line is at 5 when $x$ is zero, this is because $b = 5$.

*** Supporting example

Let's have a look at how we would use this linear model with one of the datasets: The
Boston housing prices.

#+begin_src python
import warnings
from sklearn.datasets import load_boston
with warnings.catch_warnings():
    warnings.filterwarnings("ignore")
    boston = load_boston()
boston = pd.DataFrame(
    data=np.c_[boston['data'], boston['target']],
    columns=boston['feature_names'].tolist() + ['target'])
#+end_src

#+begin_src python :results value file replace :exports results :noweb strip-export
plt.figure()
plt.scatter(boston.RM, boston.target)
plt.xlabel("Number of Rooms")
plt.ylabel(r"House Value in $1000")
<<savefig(name="images/boston_rooms_prices.png")>>
#+end_src

#+ATTR_LATEX: :width 0.6\textwidth
#+CAPTION: Scatter plot of the number of rooms in a house against the house valuation. In this plot we can see a positive effect with some outliers to this trend.
#+ATTR_ORG: :width 300px
#+RESULTS:
[[file:images/boston_rooms_prices.png]]

*** Let's fit a linear model
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END: 

We have seen that there seems to be some correlation between the number of rooms and
the house price. I.e. we can use the number of rooms of the house to get the
estimated price. To get an estimated price we'll use our linear model:

\[
y = w x + b
\]

In this case, $x$ will be the number of rooms. But what values should we set for $w$
and $b$? Or put another way, what is /optimal/ value for our model parameters.

We'll return to the question of optimal later, but for now, let's just select some
random values!

\[\begin{aligned}
w = 1 \\
b = 1
\end{aligned}\]


#+begin_src python :exports results :results value file replace :noweb strip-export
plt.figure()
plt.scatter(boston.RM, boston.target)
x = np.arange(np.min(boston.RM), np.max(boston.RM))
y = 1 * x + 1
plt.plot(x, y, 'r', label="$3 x + 5$")
plt.xlabel("Number of rooms")
plt.ylabel(r"House Value in $1000")
<<savefig(name="images/boston_rm_first_pred.png")>>
#+end_src

#+CAPTION: A linear model line overlayed onto the boston house prices dataset. Blue circles represent samples from the dataset, while the trend line is shown in red.
#+ATTR_ORG: :width 300px
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:images/boston_rm_first_pred.png]]

Well that doesn't look very good, it could be 'fit' better to what we're seeing in
the scatter plot! I wonder how wrong the linear model is -- how incorrect our
predicted house prices are?

*** Evaluating our initial linear model
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END: 

To evaluate how well, or in this case, how badly our linear model is doing, let's
compare the predicted value from the model against the actual house price. For
example, we'll take a single sample from our dataset.

If we have 4 rooms, our model estimates the house price to be $2(4) + 5 = 13$,
$13,000, but the actual cost was $24,000. This means we have underestimated the cost
by $11,000.

What we've done there is the following:

\[
\delta = | y - \hat{y} |
\]

where $\hat{y}$ is $w x + b$

We've calculated the difference or delta between the real house price $y$ and the
predicted house price.

That gives us the error for one sample though, what about for the whole dataset? Well
we could take the mean over all samples:

\[
\Delta = \sum_{i=0}^n | y - \hat{y} |
\]

#+begin_src python
y_hat = 1 * boston.RM + 1
loss  = np.mean(np.abs(boston.target - y_hat))
#+end_src

If we calculate that our linear model we see that the average difference between our
estimated value and real value is $15,000!

*** Getting better model parameters
  :PROPERTIES:
  :BEAMER_OPT: fragile,allowframebreaks,label=
  :END: 


Okay, so we made our initial guess at the model parameters (random values for $w,
b$), and these weren't very good. We were incorrectly guessing the house value by
$15,000. So how do we get better values?

Well if we visualise how badly we do vs the value for $w$ we get the following:

#+begin_src python
def lm(x, w=1, b=1):
    return w * x + b

def loss(x, y, w=1, b=1):
    return np.mean(np.abs(y - lm(x, w, b)))

ws = np.arange(-10, 15, 0.1)
ls = np.vectorize(lambda w: loss(boston.RM, boston.target, w, 1))(ws)
#+end_src

#+begin_src python :results value file replace :exports results :noweb strip-export
plt.figure()
plt.plot(ws, ls)
plt.xlabel("Value for $w$ in linear model")
plt.ylabel("MAE")
<<savefig(name="images/plot_linear_model_loss_w.png")>>
#+end_src

#+name: fig:mae_lm
#+ATTR_LATEX: :width 0.6\textwidth
#+CAPTION: Mean absolute error (MAE) between the true and predicted house values when varying the value for $w$ parameter in the linear model.
#+ATTR_ORG: :width 300px
#+RESULTS:
[[file:images/plot_linear_model_loss_w.png]]

In figure [[fig:mae_lm]], we see that as we change the $w$ parameter, the mean absolute
error (MAE), i.e. the average difference between the predicted house prices and the
true house prices, changes. Ideally, we would like the error or *loss* to be as low as
possible. In this case, when $b = 1$ the lowest possible loss we can hope to achieve
with the linear model is ~ $5,500.

But what value for $w$ gets us this lowest value for the loss? Looking at the graph,
we see that the lowest point on the loss curve is somewhere between 0 and 5. Maybe
even 4? While we could look at the curve and pick these parameter values, we're going
to use a better method -- one that give us an optimal value for this loss curve
automatically.

We're going to look at the method called *Gradient Descent*.

If we visualise our loss curve again, and visualise where $w = 1$ is on this curve,
we will see:

#+begin_src python :results value file replace :exports results :noweb strip-export
plt.figure()
plt.plot(ws, ls, label="loss over $w$")
plt.xlabel("$w$")
plt.ylabel("$MAE$")
plt.scatter([1], [loss(boston.RM, boston.target, 1, 1)], color="red", label="when $w = 1$")
plt.legend()
<<savefig(name="images/loss_curve_w_1.png")>>
#+end_src

#+ATTR_ORG: :width 300px
#+RESULTS:
[[file:images/loss_curve_w_1.png]]

So we want this rot dot to move down the loss curve and reach the bottom of the
curve. Using the *Gradient Descent* algorithm, we're going to take *very small steps*
down the loss curve.

#+ATTR_ORG: :width 300px
#+ATTR_LATEX: :width 0.6\textwidth
[[file:images/loss_curve_w_1_with_path.png]]

To determine which way is up, and which way is down the curve, we use the *Gradient* of
the curve (hence Gradient Descent). We compute the gradient using finite differences method:

\[
\Delta = \frac{f(x+h) - f(x)}{h}
\]

where $f(x)$ is the loss when $w$ takes on the value of $x$. $h$ is a very small
value.

#+begin_src python :results value file replace :exports results :noweb strip-export
from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, mark_inset
fig, ax = plt.subplots()
ax.plot(ws, ls)
ax.scatter([1], [loss(boston.RM, boston.target, 1, 1)], color='red')

# zoomed axes
axins = zoomed_inset_axes(ax, 8, loc=1)
axins.plot(ws, ls)
axins.scatter([1], [loss(boston.RM, boston.target, 1, 1)], color='red')
axins.set_ylim(11, 17)
axins.set_xlim(0, 2)

# measuring gradient
axins.plot([1, 1.5], [loss(boston.RM, boston.target, 1.5, 1), loss(boston.RM, boston.target, 1.5, 1)], color='black')
axins.plot([1, 1], [loss(boston.RM, boston.target, 1, 1), loss(boston.RM, boston.target, 1.5, 1)], color='black')
axins.text(1.23, loss(boston.RM, boston.target, 1.58, 1), "$h$")
axins.text(0.6, loss(boston.RM, boston.target, 1.30, 1), "$f(x+h)$")


plt.xticks(visible=False)
plt.yticks(visible=False)
mark_inset(ax, axins, loc1=2, loc2=4, fc="none", ec="0.5")
<<savefig(name="images/small_step.png")>>
#+end_src

#+ATTR_ORG: :width 300px
#+RESULTS:
[[file:images/small_step.png]]

If we select $h = 0.5$ then we will have the formula:

\[
\Delta_w = \frac{\mathcal{L}(w + 0.5) - \mathcal{L}(w)}{w}
\]

where $\mathcal{L}$ represents our loss function, MAE. If we calculate this we have:
#+LaTeX_header: \usepackage{amsmath}


\[\begin{aligned}
\Delta_w &= \frac{\mathcal{L}(w + 0.5) - \mathcal{L}(w)}{h} \\
&= \frac{\mathcal{L}(1.5)- \mathcal{L}(1)}{1  - (0.5)} \\
&= \frac{12 - 15}{0.5} \\
&= -6.0
\end{aligned}\]

Given that the gradient is a negative number, we know that the curve is going
down/decreasing. So we will want to move $w$ in this direction -- we want to move
$w$ so that the loss decreases.

\[
\overline{w} = w - \eta \Delta_w
\]

If we plug in the numbers we've calculated for when $w = 1$ we get and $eta = 0.5$:

\[\begin{aligned}
\overline{w} &= w - \eta \Delta_w \\
&= 1.0 - (0.5 * -6.0) \\
&= 1.0 - (-3.0) \\
&= 4.0
\end{aligned}\]

Our new value for the $w$ parameter ($\overline{w}$) is computed by taking its
original value and subtracting the gradient modulated/multiplied by $\eta$. $\eta$ in
this case is what will allow us to take our *small steps*. It is important to set
$\eta$ to a suitably small value, as high values for $\eta$ will cause the Gradient
Descent to behave erratically, and even, make our loss worse!

#+begin_src python :results value file replace :exports results :noweb strip-export
fig, ax = plt.subplots(1, 3, figsize=(8, 3))
w = -5
ax[0].plot(ws, ls)
ax[0].scatter([w], [loss(boston.RM, boston.target, w, 1)], color='red')
ax[1].plot(ws, ls)
ax[1].scatter([w], [loss(boston.RM, boston.target, w, 1)], color='red')
ax[2].plot(ws, ls)
ax[2].scatter([w], [loss(boston.RM, boston.target, w, 1)], color='red')

# axes1
def grad(ls_fn, w, h=1e-5):
    return (ls_fn(w) - ls_fn(w+h)) / (w - (w+h))

def update(w, eta=0.001):
    g = grad(lambda w: loss(boston.RM, boston.target, w, 1), w)
    return w - eta * g

def learn(eta):
    w = -5
    losses = []
    w_vals = []
    l = loss(boston.RM, boston.target, w, 1)
    losses.append(l)
    w_vals.append(w)
    for i in range(10):
        w = update(w, eta=eta)
        l = loss(boston.RM, boston.target, w, 1)
        losses.append(l)
        w_vals.append(w)
    return losses, w_vals

eta = 0.05
losses, w_vals = learn(eta)
ax[0].plot(w_vals, losses, 'x--', color='green')
ax[0].set_title(f"$\eta = {eta}$")

eta = 0.3
losses, w_vals = learn(eta)
ax[1].plot(w_vals, losses, 'x--', color='green')
ax[1].set_title(f"$\eta = {eta}$")

eta = 3
losses, w_vals = learn(eta)
ax[2].plot(w_vals, losses, 'x--', color='green')
ax[2].set_title(f"$\eta = {eta}$")

<<savefig(name="images/small_step_eta.png")>>
#+end_src

#+caption: Plotting the effect of $\eta$ on the step change of $w$.
#+name: fig:eta
#+ATTR_ORG: :width 300px
#+RESULTS:
[[file:images/small_step_eta.png]]

In figure [[fig:eta]], we've varied the value of $\eta$ and computed 10 steps of updating
the $w$ parameter in our linear model. When $\eta=0.05$, we see that $w$ is slowly
being updated in a way that is causing our loss to decrease, but it is more so slowly
that we don't reach the optimal value for $w$. When $\eta=3$, each change in $w$ is
too large, so we over-shoot the optimal value, and end up bouncing back and forth
without ever improving. Finally, when we set $\eta=0.3$, the changes in $w$ are
sufficiently large enough such that we reach the *global minima* in time, but they are
also small enough so that we don't over-shoot this same minimum.

#+begin_src python
import random

class LinearModel:
    def __init__(self):
        self.w = random.randint(1, 1)
        self.b = 1
        self.w_states = [self.w]  # for visualisations
        self.b_states = [self.b] 
    def set_w(self, new_w):
        self.w = new_w
        self.w_states.append(new_w)
    def set_b(self, new_b):
        self.b = new_b
        self.b_states.append(new_b)
    @property
    def params(self):
        return self.w, self.b
    def __call__(self, x, w=None, b=None):
        if w is None: w = self.w
        if b is None: b = self.b
        return w * x + b

def mae(y, y_hat):
    return np.mean(np.abs(y - y_hat))

def grad(f, param, diff=1e-7):
    return (f(param+diff) - f(param)) / diff

def update(g, gradient, eta=0.01):
    return g - eta * gradient

def fit(f, x, y, loss_fn, learning_rate=0.5, epochs=10):
    losses = []
    for i in range(epochs):
        l = loss_fn(y, f(x))
        w, b = f.params
        d_w = grad(lambda p: loss_fn(y, f(x, w=p)), w)
        d_b = grad(lambda p: loss_fn(y, f(x, b=p)), b)
        f.set_w(update(f.w, d_w, learning_rate))
        f.set_b(update(f.b, d_b, learning_rate))
        losses.append(l)
    return losses
        

lm = LinearModel()
losses = fit(lm, boston.RM, boston.target, mae, learning_rate=0.05, epochs = 50)
#+end_src

#+begin_src python
from matplotlib.animation import FuncAnimation
fig, ax = plt.subplots()
ax.scatter(boston.RM, boston.target)
ax.set_xlabel("Number of rooms")
ax.set_ylabel(r"House valuation in $1000")
lines = ax.plot([], 'r--')
line = lines[0]
title = ax.text(7.5, 5, "")
def anim(frame):
    w, b = lm.w_states[frame], lm.b_states[frame]
    line.set_data(boston.RM, lm(boston.RM, w, b))
    title.set_text(f"$w={w:.2f}, b={b:.2f}$")
anim_created = FuncAnimation(fig, anim, frames=len(lm.w_states), interval=1)
#anim_created.save("./images/lm_learn.gif", writer="imagemagick", fps=10)
#+end_src

If we then apply the Gradient Descent algorithm to both parameters of the linear
model $w, b$, then we can find the optimal trend line for this data. Furthermore,
visualising this will look something like figure [[fig:lm_learn]].

#+ATTR_ORG: :width 300px
#+ATTR_LATEX: :width 0.5\textwidth
#+name: fig:lm_learn
#+CAPTION: Visualising (GIF) the linear model as its parameters improve.
[[file:images/lm_learn.gif]]


*** Multiple variables

* Logistic Regression

** Classification

*** Moving from regression to classification

We now turn to classification

# Local Variables:
# org-latex-minted-options: (("frame" "lines") ("linenos=true") ("firstnumber=last") ("fontsize=\\footnotesize") ("bgcolor=LightGray") ("xleftmargin=5pt") ("tabsize=2") ("breaklines=true") ("numbersep=10pt"))
# End:
